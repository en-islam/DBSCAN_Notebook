{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Volcanic activity detection and noise characterization using machine learning \n",
    "#### Using Density-Based Clustering to Find Relationaships in Noise"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Author(s)\n",
    "\n",
    "\n",
    "- Author1 = {\"name\": \"Myles Mason\", \"affiliation\": \"Virginia Tech\", \"email\": \"mylesm18@vt.edu\", \"orcid\": \"0000-0002-8811-8294\"}\n",
    "- Author2 = {\"name\": \"John Wenskovitch\", \"affiliation\": \"Virginia Tech\", \"email\": \"jw87@vt.edu\", \"orcid\": \"0000-0002-0573-6442\"}\n",
    "- Author3 = {\"name\": \"D. Sarah Stamps\", \"affiliation\": \"Virginia Tech\", \"email\": \"dstamps@vt.edu\",\"orcid\": \"0000-0002-3531-1752\"}\n",
    "- Author4 = {\"name\": \"Joshua Robert Jones\", \"affiliation\": \"Virginia Tech\", \"email\": \"joshj55@vt.edu\", \"orcid\": \"0000-0002-6078-4287\"}\n",
    "- Author5 = {\"name\": \"Mike Dye\", \"affiliation\": \"Unaffiliated\", \"email\": \"mike@mikedye.com\", \"orcid\": \" 0000-0003-2065-870X\"}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Purpose\n",
    "This Jupyter notebook explores methods towards characterizing noise and eventually predicting volcanic activity for [Ol Doinyo Lengai](http://tzvolcano.chordsrt.com)\n",
    " (an active volcano in Tanzania) with machine learning. Machine learning is a powerful tool that enables the automatization of complex mathematical and analytical models. In this Jupyter notebook, the components are time and height. This project uses Global Navigation Satellite System (GNSS) data from the EarthCube CHORDS portal TZVOLCANO (Stamps et al. 2016; Daniels et al., 2016; Kerkez et al., 2016), which is the online interface for obtaining open-access real-time positioning data collected around Ol Doinyo Lengai(http://tzvolcano.chordsrt.com). The bulk of the project is the exploration of the data and later prediction of height points. The station that this project analyzes is OLO1 for days 12/16/2020 and 04/16/2021.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Methodology\n",
    "The desired data was imported and selected. Data was then converted to a numerical metric. The optimal epsilon value was found and then the data was scaled. Finally, denisty-based clustering with DBSCAN to find trends among the data at various minimum samples and number of neighbors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Importing and setup "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import Libraries\n",
    "# Data manipulation\n",
    "import pandas as pd\n",
    "import json\n",
    "import numpy as np\n",
    "import math\n",
    "from datetime import datetime as dt\n",
    "\n",
    "\n",
    "# Visualizations\n",
    "%matplotlib inline\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "# Modeling\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: '2021-01-01_1_tzv.json'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-2-c7e4c1b48045>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[1;32mwith\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'2021-01-01_1_tzv.json'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m'r'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"utf-8\"\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0minfile\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m     \u001b[0mtzList\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mjson\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0minfile\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      3\u001b[0m \u001b[0mtzDF\u001b[0m \u001b[1;33m=\u001b[0m  \u001b[0mpd\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mjson_normalize\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtzList\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"features\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"properties\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m\"data\"\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0msep\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m'_'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;32mdef\u001b[0m \u001b[0mtimeconvertfunc\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtimestamp\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m  \u001b[1;31m#The format of tzDF[\"measurments_height] is in string form and timeseries so this method make it an integer\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '2021-01-01_1_tzv.json'"
     ]
    }
   ],
   "source": [
    "with open('2021-01-01_1_tzv.json', 'r', encoding=\"utf-8\") as infile:\n",
    "    tzList = json.load(infile)\n",
    "tzDF =  pd.json_normalize(tzList[\"features\"][0][\"properties\"][\"data\"], sep='_')\n",
    "def timeconvertfunc(timestamp):\n",
    " #The format of tzDF[\"measurments_height] is in string form and timeseries so this method make it an integer\"\n",
    "  ts = pd.Timestamp(timestamp, tz=None).to_pydatetime()\n",
    "  ts = 3600*ts.hour + 60*ts.minute + ts.second\n",
    "\n",
    "  return ts\n",
    "#Applying above method to the two data frames\n",
    "tzDF[\"timeconvert\"] = tzDF[\"time\"].apply(timeconvertfunc)\n",
    "#tzCleanDF[\"timeconvert\"] = tzCleanDF[\"time\"].apply(timeconvertfunc)\n",
    "tzDF"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Finding Optimal Episilon value \n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "'''\n",
    "X = tzDF[\"measurements_height\"]\n",
    "X = X.values.reshape(1,-1)\n",
    "neigh = NearestNeighbors(n_neighbors=2)\n",
    "nbrs = neigh.fit(X)\n",
    "distances, indices = nbrs.kneighbors(X)\n",
    "\n",
    "#distances = np.sort(distances, axis=0)\n",
    "#distances = distances[:,1]\n",
    "#plt.plot(distances)\n",
    "distances\n",
    "'''\n",
    "#Scale the data before applying chunk below \n",
    "max_tzDF = max(tzDF[\"measurements_height\"])\n",
    "min_tzDF = min(tzDF[\"measurements_height\"])\n",
    "mean_tzDF = tzDF[\"measurements_height\"].mean()\n",
    "\n",
    "dif_Mean_Max = mean_tzDF - max_tzDF\n",
    "dif_Mean_Min = mean_tzDF - min_tzDF\n",
    "min_Mean_Differnces = min(dif_Mean_Max,dif_Mean_Min)\n",
    "\n",
    "#dif_Mean_Max is the optimal episllon value with this approach \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Scale data\n",
    "from numpy import asarray\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "\n",
    "pre_Scale_Height = tzDF[\"measurements_height\"].values.reshape(-1,1)\n",
    "scaler = MinMaxScaler()\n",
    "scaled = scaler.fit_transform(pre_Scale_Height)\n",
    "scaled[0][0]\n",
    "\n",
    "\n",
    "\n",
    "neigh = NearestNeighbors(n_neighbors=2)\n",
    "nbrs = neigh.fit(scaled)\n",
    "distances, indices = nbrs.kneighbors(scaled) \n",
    "distances = np.sort(distances, axis=0)\n",
    "distances = distances[:,1]\n",
    "plt.plot(distances)\n",
    "max(distances)\n",
    "\n",
    "\n",
    "# Max occures when x = 51273\n",
    "\n",
    "print(np.argmax(distances,axis=0))\n",
    "#distances\n",
    "\n",
    "#distances\n",
    "#np.argmax(a, axis=1)\n",
    "'''\n",
    "max_tzDF = max(scaled)\n",
    "min_tzDF = min(scaled)\n",
    "mean_tzDF = scaled.mean()\n",
    "dif_Mean_Max = mean_tzDF - max_tzDF\n",
    "dif_Mean_Min = mean_tzDF - min_tzDF\n",
    "min_Mean_Differnces = min(dif_Mean_Max,dif_Mean_Min)\n",
    "print(min_Mean_Differnces)\n",
    "'''\n",
    "distances = np.sort(distances, axis=0)\n",
    "#distances = distances[distances[:,1]]\n",
    "#plt.plot(distances)\n",
    "\n",
    "#distances.shape()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#NEAREST NEIGHBOR\n",
    "from sklearn.neighbors import NearestNeighbors\n",
    "from numpy import asarray\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "sns.set()\n",
    "pre_Scale_Height = tzDF[\"measurements_height\"].values.reshape(-1,1)\n",
    "\n",
    "scaler = MinMaxScaler()\n",
    "\n",
    "scaled = scaler.fit_transform(pre_Scale_Height)\n",
    "nbrs = NearestNeighbors(n_neighbors=3, metric='cosine').fit(scaled)\n",
    "distances, indices = nbrs.kneighbors(scaled)\n",
    "m = DBSCAN(eps=0.55408633, min_samples=4).fit(distances)\n",
    "\n",
    "#clusters = m.lables_\n",
    "\n",
    "colors = ['royalblue', 'maroon', 'forestgreen', 'mediumorchid', 'tan', 'deeppink', 'olive', 'goldenrod', 'lightcyan', 'navy']\n",
    "vectorizer = np.vectorize(lambda x: colors[x % len(colors)])\n",
    "#plt.scatter(X[:,0], X[:,1], c=vectorizer(clusters))\n",
    "\"\"\"\n",
    "distances = distances[:,2]\n",
    "distances = np.sort(distances, axis=0)\n",
    "plt.plot(distances)\n",
    "plt.show()\n",
    "distances[0].max()\n",
    "indices\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Applying DBSCAN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.neighbors import NearestNeighbors\n",
    "from numpy import asarray\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.cluster import DBSCAN\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "import seaborn as sns\n",
    "#Scaling\n",
    "pre_Scale_Height = tzDF[\"measurements_height\"].values.reshape(-1,1)\n",
    "pre_Scale_Time = tzDF[\"timeconvert\"].values.reshape(-1,1)\n",
    "scaled = scaler.fit_transform(pre_Scale_Height)\n",
    "scaled_Two = scaler.fit_transform(pre_Scale_Time)\n",
    "\"\"\"\"\n",
    "args:\n",
    "A dataframe, number of neighbors, minmium sample points, and an \n",
    "episilon value for the nearest neighbor algorithm to be executed.\n",
    "return:\n",
    "    Nothing a DBSCAN algorithm is shown on the dataset graphically.\n",
    "\"\"\"\n",
    "\n",
    "def make_DBSCAN(DataFrame,numNeigh,min_Samp,Episilon):\n",
    "    #Input for the algorithm\n",
    "    \n",
    "    \n",
    "    #Scaling the height of the data between 0 & 1\n",
    "    scaler = MinMaxScaler()\n",
    "\n",
    "    scaled_Two = scaler.fit_transform(pre_Scale_Time)\n",
    "    nbrs = NearestNeighbors(n_neighbors = numNeigh).fit(scaled)\n",
    "    distances, indices = nbrs.kneighbors(scaled) \n",
    "\n",
    "    distances = np.sort(distances, axis=0)\n",
    "    distances = distances[:,4]\n",
    "    \n",
    "    db = DBSCAN(eps = Episilon,  min_samples = min_Samp)\n",
    "    db.fit(scaled)\n",
    "    y_pred = db.fit_predict(scaled)\n",
    "    plt.figure(figsize=(20,12))\n",
    "    plt.title(\"DBSCAN\")\n",
    "    plt.scatter(pre_Scale_Time,pre_Scale_Height,c=y_pred, cmap='Paired',s=10)\n",
    "    \n",
    "   \n",
    "    \n",
    "#One common approach is to make minPoints = log(number of points)\n",
    "ln_Data_Points= np.log(5990)\n",
    "\n",
    "\n",
    "#DBSCAN (#eps,min_samples)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of neighbors varying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Five neighbors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "make_DBSCAN(tzDF,5,50,0.001)\n",
    "plt.ylabel(\"Height\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ten neighbors"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "make_DBSCAN(tzDF,10,50,0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fifteen neighbors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "make_DBSCAN(tzDF,15,50,0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twenty neighbors "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_DBSCAN(tzDF,20,50,0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of min_samples varying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Five samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_DBSCAN(tzDF,5,5,0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ten samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_DBSCAN(tzDF,5,10,0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fifteen samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_DBSCAN(tzDF,5,15,0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Twenty-five samples"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "make_DBSCAN(tzDF,25,50,0.001)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Number of eps varying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10^-1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_DBSCAN(tzDF,5,5,0.1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10^-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_DBSCAN(tzDF,5,5,0.01)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10^-3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_DBSCAN(tzDF,5,5,0.001)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 10^-4"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "make_DBSCAN(tzDF,5,5,0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "make_DBSCAN(tzDF,5,25,0.0001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
